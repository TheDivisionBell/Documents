{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed2e4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec55be6c-d3d1-48e3-98c9-0b1fd16fdf4a",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Machine Learning: relies on hand crafted feature engineering\n",
    "\n",
    "Deep Learning: enables feature learning from raw data. It is a subset of machine learning and requires a lot of data. It can handle:\n",
    "- torchvision: images\n",
    "- torchaudio: audio\n",
    "- torchtext: text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25908398-a94b-4696-b718-5358ff571746",
   "metadata": {},
   "source": [
    "### Tensor definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53ada601-0abf-4d1b-bf94-9f33a09eea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from list\n",
    "input_list = [22,46,55,6,7,81,54,6,100]\n",
    "input_t = torch.tensor(input_list)\n",
    "\n",
    "# from numpy array\n",
    "input_np = np.array([23,44,56,78,44,64,98])\n",
    "input_t_np = torch.from_numpy(input_np)\n",
    "\n",
    "# random\n",
    "input_t_rand = torch.rand(3,3)\n",
    "\n",
    "# zeros\n",
    "input_t_zeros = torch.zeros(2,3)\n",
    "\n",
    "# ones\n",
    "input_t_ones = torch.ones(3,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04092b-97a1-441d-82d0-133f1fa9bf69",
   "metadata": {},
   "source": [
    "### Tensor attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b90976e8-94a4-4777-950f-27e03dfd649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([9]), Type: torch.int64, Device: cpu\n"
     ]
    }
   ],
   "source": [
    "## attributes\n",
    "shape = input_t.shape\n",
    "type = input_t.dtype\n",
    "device = input_t.device\n",
    "\n",
    "print(f'Shape: {shape}, Type: {type}, Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3743a77-d56a-4a1a-b935-3e0ce8eb9e30",
   "metadata": {},
   "source": [
    "### First Neural Network\n",
    "\n",
    "Define a neural Network with one linear layer which takes:\n",
    "- Input of size n\n",
    "- Applies a linear function\n",
    "- Return an output with size m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4f1a675-9797-4d05-a6be-e013824bb719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Weigth: Parameter containing:\n",
      "tensor([[-0.0459,  0.0295,  0.2518],\n",
      "        [ 0.4441,  0.5070,  0.0743]], requires_grad=True), \n",
      " Bias: Parameter containing:\n",
      "tensor([0.5265, 0.1087], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# First Neural Network with 3 features as input and 2 node as output\n",
    "input_tensor = torch.tensor([[0.3471, 0.457, -0.2356]])\n",
    "# initialize the linear layer\n",
    "linear_layer = nn.Linear(in_features=3, out_features=2)\n",
    "print(f' Weigth: {linear_layer.weight}, \\n Bias: {linear_layer.bias}')\n",
    "# generate output\n",
    "output = linear_layer(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42a133aa-1359-4e6d-8695-208495e652a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking multiple layers: create network with three linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10,18),\n",
    "    nn.Linear(18, 20),\n",
    "    nn.Linear(20,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19083133-bbe9-4ca6-989a-e8bd6a879c53",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "\n",
    "An activation function add the non linearity inside the neural network. In this way a model can learn more complex relationship. Otherwise the output of many W @ x + b would be a linear function hence many layers could still be summarized with only one.\n",
    "- Sigmoid Function: used in binary classification. Takes an input and gives an output between 0-1 which can be interpreted as a probability with a threshold of 0.5. This function is $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "- softmax:  is a mathematical function that takes as input a vector of arbitrary real-valued numbers and normalizes it into a probability distribution consisting of values between 0 and 1 that sum to 1. It's commonly used in machine learning for multi-class classification problems $$ y_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} $$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a1d1acc-ad4c-42c2-93b7-f9acaafbb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_sigmoid = nn.Sequential(\n",
    "    nn.Linear(5,4),\n",
    "    nn.Linear(4, 1),\n",
    "    nn.Sigmoid() # if not specified it takes the last input which should be 1-D\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4149e464-8db7-403c-82b2-200c531ae75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_softmax = nn.Sequential(\n",
    "    nn.Linear(5,4),\n",
    "    nn.Linear(4, 1),\n",
    "    nn.Softmax(dim=-1) # indicates that the softmax is applied along the last dimension of the input tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de71863-2941-4491-92c6-a994e2aedabb",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "- Input data is passed forward or propagated through a network\n",
    "- Computations are performed at each layer\n",
    "- Output of each layer passed to each subsequential layer\n",
    "- Output of final layer: \"prediction\"\n",
    "  \n",
    "It is used both for training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb55b4-14f9-4de1-a4b6-171e0cf8fc0e",
   "metadata": {},
   "source": [
    "### Backward pass (Backpropagation)\n",
    "\n",
    "It is used to update weights and biases during training. it is a complementary step to the forward pass. More in general:\n",
    "1. Propagate data forward\n",
    "2. Compare outouts to true values (ground truth)\n",
    "3. Backpropagate to update model and biases\n",
    "4. Repeat until weights and biases are tuned to produce useful outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dec1ae78-1ac3-4438-9059-4d6abcf709b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_ex = torch.rand(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe4050d0-879d-4165-98e4-dd534a0e461a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_ex.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474ab436-6169-4bd8-8c5c-57adebd6be96",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d99d2a3-30ca-44ce-bf8b-e69e68725906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classification Output tensor([[0.6717]], grad_fn=<SigmoidBackward0>)\n",
      "Binary Classification Output tensor([[0.8205, 0.0070, 0.1243, 0.0482]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Binary Classification\n",
    "binary = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "model_binary = nn.Sequential(\n",
    "    nn.Linear(8,4),\n",
    "    nn.Linear(4,3),\n",
    "    nn.Linear(3,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output_binary = model_binary(binary)\n",
    "print(f'Binary Classification Output {output_binary}')\n",
    "\n",
    "\n",
    "# Multi Class Classification\n",
    "n_classes = 4\n",
    "multiclass = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "model_multiclass = nn.Sequential(\n",
    "    nn.Linear(8,4),\n",
    "    nn.Linear(4,3),\n",
    "    nn.Linear(3,4),\n",
    "    nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output_multiclass = model_multiclass(multiclass)\n",
    "print(f'Binary Classification Output {output_multiclass}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e9288f-9463-405f-b04e-6b242bd17266",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "- gives feedback to the model during training\n",
    "- takes in model prediction $ \\hat{y_i} $ and ground truth $ y_i $\n",
    "- Output a float\n",
    "\n",
    "It must be highlighted that $ y_i $ is a single integer (the class label) while $ \\hat{y_i} $ is a tensor (output of the softmax function) that might be of different sizes.  \n",
    "\n",
    "To compare an integer to a tensor we use one-hot encoding where the ground truth is returned as a tensor with 1 in the class it is representing and 0 o the others. In this way we allow the calculation between the output and the ground truth. The conversion of the labels in tensors is done by torch.nn.functional.one_hot()\n",
    "\n",
    "Among others we have the following loss functions:\n",
    "1. Cross Entropy: for classification problems $\r\n",
    "\\text{Cross-Entropy Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right)$\\]\n",
    "2. Mean Square Error MSE: $ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c6b2ecb-15fa-4231-b77e-7a5958aa8f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor from a label where we want the first class to be true\n",
    "import torch.nn.functional as F\n",
    "F.one_hot(torch.tensor(0), num_classes =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ce4bb6a-2aee-4702-95fd-9c99dbb9742a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8130, dtype=torch.float64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declare the loss function\n",
    "from torch.nn import CrossEntropyLoss\n",
    "scores = torch.tensor([[-0.121, 0.1059]])\n",
    "one_hot_target = torch.tensor([[1,0]])\n",
    "criterion = CrossEntropyLoss()\n",
    "criterion(scores.double(), one_hot_target.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995b373-531c-4a3e-b5ba-3ef68cc50a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
